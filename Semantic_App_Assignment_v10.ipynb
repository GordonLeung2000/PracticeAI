{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-539bd27a48c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPyPDF2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPdfReader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbertopic\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bertopic\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbertopic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bertopic\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0m__version__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bertopic\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bertopic\\_bertopic.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# Models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mhdbscan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mumap\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mUMAP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hdbscan\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mhdbscan_\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHDBSCAN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdbscan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mrobust_single_linkage_\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRobustSingleLinkage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrobust_single_linkage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mvalidity\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvalidity_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m from .prediction import (approximate_predict,\n\u001b[0;32m      5\u001b[0m                          \u001b[0mmembership_vector\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hdbscan\\hdbscan_.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsgraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m from ._hdbscan_linkage import (\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0msingle_linkage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mmst_linkage_core\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mhdbscan\\\\_hdbscan_linkage.pyx\u001b[0m in \u001b[0;36minit hdbscan._hdbscan_linkage\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it)."
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from openai import AzureOpenAI\n",
    "import os\n",
    "import json\n",
    "from PyPDF2 import PdfReader\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[2]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Fetch and Process Policy Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_policy_content():\n",
    "    \"\"\"Fetch the content of the HKUST policy document.\"\"\"\n",
    "    url = \"https://legal.hkust.edu.hk/files/Policy_Use_of_University_Titles_Names_and_Logos.pdf\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(\"policy.pdf\", \"wb\") as file:\n",
    "                file.write(response.content)\n",
    "            return \"policy.pdf\"\n",
    "        else:\n",
    "            st.error(f\"Failed to fetch the document. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        st.error(f\"An error occurred while fetching the document: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        st.error(f\"An error occurred while extracting text from the PDF: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[3]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = AzureOpenAI(\n",
    "  api_key = \"f21c398476b146988044b391bd5256df\", # use your key here\n",
    "  api_version = \"2024-06-01\", # apparently HKUST uses a deprecated version\n",
    "  azure_endpoint = \"https://hkust.azure-api.net\" # per HKUST instructions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text_list, model=\"text-embedding-ada-002\"):\n",
    "    \"\"\"Get embeddings for a list of texts.\"\"\"\n",
    "    try:\n",
    "        response = openai_client.embeddings.create(\n",
    "            input=text_list,\n",
    "            model=model,\n",
    "            encoding_format=\"float\",\n",
    "        )\n",
    "        return [data.embedding for data in response.data]\n",
    "    except Exception as e:\n",
    "        st.error(f\"An error occurred while generating embeddings: {e}\")\n",
    "        return None\n",
    "\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[4]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[5]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_relevant_section(query_embedding, paragraph_embeddings, paragraphs, topics):\n",
    "    \"\"\"Perform a simple semantic search to find the relevant section in the policy document.\"\"\"\n",
    "    similarities = [cosine_similarity(query_embedding, paragraph_embedding) for paragraph_embedding in paragraph_embeddings]\n",
    "\n",
    "    if not similarities:\n",
    "        st.error(\"No valid paragraph embeddings were generated.\")\n",
    "        return None, None\n",
    "\n",
    "    most_relevant_index = np.argmax(similarities)\n",
    "    most_relevant_score = similarities[most_relevant_index]\n",
    "\n",
    "    if most_relevant_score < 0.5:  # Set a threshold for relevance\n",
    "        return None, most_relevant_score\n",
    "\n",
    "    return paragraphs[most_relevant_index], most_relevant_score, topics[most_relevant_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BERTopic to define topics\n",
    "def define_topics(paragraphs):\n",
    "    \"\"\"Use BERTopic to extract topics from paragraphs.\"\"\"\n",
    "    topic_model = BERTopic()\n",
    "    topics, _ = topic_model.fit_transform(paragraphs)\n",
    "    return topic_model.get_topic_info(), topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[6]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streamlit App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlit app\n",
    "def main():\n",
    "    st.title(\"HKUST Policy Q&A\")\n",
    "    st.write(\"Ask questions about the HKUST policy document.\")\n",
    "\n",
    "    if \"policy_content\" not in st.session_state:\n",
    "        st.session_state[\"policy_content\"] = None\n",
    "    if \"policy_embeddings\" not in st.session_state:\n",
    "        st.session_state[\"policy_embeddings\"] = None\n",
    "    if \"policy_topics\" not in st.session_state:\n",
    "        st.session_state[\"policy_topics\"] = None\n",
    "\n",
    "    if st.session_state[\"policy_content\"] is None:\n",
    "        pdf_path = fetch_policy_content()\n",
    "        if pdf_path:\n",
    "            content = extract_text_from_pdf(pdf_path)\n",
    "            if content:\n",
    "                st.session_state[\"policy_content\"] = content\n",
    "                paragraphs = content.split(\"\\n\\n\")  # Splitting by double newlines\n",
    "                st.session_state[\"policy_embeddings\"] = get_embeddings(paragraphs)\n",
    "                topic_info, topics = define_topics(paragraphs)\n",
    "                st.session_state[\"policy_topics\"] = topics\n",
    "                st.session_state[\"topic_info\"] = topic_info\n",
    "\n",
    "    user_query = st.text_input(\"Your Question:\", placeholder=\"Type your question here...\")\n",
    "    if st.button(\"Submit\"):\n",
    "        if user_query:\n",
    "            query_embedding = get_embeddings([user_query])\n",
    "            if query_embedding and st.session_state[\"policy_content\"]:\n",
    "                most_relevant_section, similarity_score, relevant_topic = search_relevant_section(\n",
    "                    query_embedding[0],  # Use the first (and only) embedding\n",
    "                    st.session_state[\"policy_embeddings\"],\n",
    "                    st.session_state[\"policy_content\"].split(\"\\n\\n\"),\n",
    "                    st.session_state[\"policy_topics\"]\n",
    "                )\n",
    "                if most_relevant_section:\n",
    "                    st.subheader(\"Response:\")\n",
    "                    st.write(f\"**Topic**: {relevant_topic}\")\n",
    "                    st.write(most_relevant_section)\n",
    "                    st.write(f\"**Relevance Score**: {similarity_score:.2f}\")\n",
    "                else:\n",
    "                    st.write(\"No relevant section found.\")\n",
    "            else:\n",
    "                st.error(\"An error occurred while processing your query.\")\n",
    "        else:\n",
    "            st.error(\"Please type a question!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
